{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Imbalanced Dataset with Machine Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*We need to find out from the dataaset of creditcard whether this transactions fraud or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #It has 284807 rows and 31 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #tHERE ARE NO NULL VALUES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      284807\n",
       "V1        284807\n",
       "V2        284807\n",
       "V3        284807\n",
       "V4        284807\n",
       "V5        284807\n",
       "V6        284807\n",
       "V7        284807\n",
       "V8        284807\n",
       "V9        284807\n",
       "V10       284807\n",
       "V11       284807\n",
       "V12       284807\n",
       "V13       284807\n",
       "V14       284807\n",
       "V15       284807\n",
       "V16       284807\n",
       "V17       284807\n",
       "V18       284807\n",
       "V19       284807\n",
       "V20       284807\n",
       "V21       284807\n",
       "V22       284807\n",
       "V23       284807\n",
       "V24       284807\n",
       "V25       284807\n",
       "V26       284807\n",
       "V27       284807\n",
       "V28       284807\n",
       "Amount    284807\n",
       "Class     284807\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().count() #ALL R HAVING SAME RECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts() #tHIS IS MY DEPENDENT FEATURE.\n",
    "#U have so many records this is completely imbalanced. There is huge difference btw this two(0 & 1)\n",
    "#We will try to see how to solve this in particular way.\n",
    "#There r many ways to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Amount  \n",
       "0       149.62  \n",
       "1         2.69  \n",
       "2       378.66  \n",
       "3       123.50  \n",
       "4        69.99  \n",
       "...        ...  \n",
       "284802    0.77  \n",
       "284803   24.79  \n",
       "284804   67.88  \n",
       "284805   10.00  \n",
       "284806  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"Class\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What are dependent and independent features in Machine Learning\n",
    "\n",
    "In Machine Learning, understanding the distinction between dependent and independent features is crucial. Here's a breakdown:\n",
    "\n",
    "Independent Features (also called predictors or input variables):\n",
    "\n",
    "Represent the inputs fed into the machine learning model.\n",
    "Not influenced by the dependent variable.\n",
    "Used by the model to learn patterns and make predictions about the dependent variable.\n",
    "Example: In a model predicting house prices, features like square footage, number of bedrooms, and neighborhood could be independent variables.\n",
    "\n",
    "Dependent Feature (also called target variable or output variable):\n",
    "\n",
    "Represents the output or what the model is trying to predict.\n",
    "Impacted by the changes in the independent variables.\n",
    "The model's goal is to learn the relationship between the independent features and the dependent variable.\n",
    "Example: In the house price prediction model, the dependent variable would be the actual selling price of the house.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Supervised learning relies heavily on these concepts. The model is trained on data with both independent and dependent variables.\n",
    "Unsupervised learning doesn't involve a dependent variable. The model focuses on identifying patterns within the data itself.\n",
    "In essence:\n",
    "\n",
    "Think of independent features as the causes and the dependent feature as the effect.\n",
    "The model learns how the causes (independent features) influence the effect (dependent feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Independent and Dependent Features\n",
    "X=df.drop(\"Class\",axis=1)\n",
    "y=df.Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Like KFOLD and Hyperpaqrameter Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*During imbalance dataset do not check jst the accuracy\n",
    "\n",
    "*Cross Validation is also one of the way which actually helps u to handle the imbalance dataset coz ur model will be trained on sample of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression Explain this in detail and for what it is used?\n",
    "\n",
    "The code snippet from sklearn.linear_model import LogisticRegression imports a specific machine learning model from the scikit-learn library in Python. Let's break down what this means:\n",
    "\n",
    "1. scikit-learn:\n",
    "\n",
    "A popular open-source library in Python for machine learning tasks.\n",
    "Provides a wide range of tools and algorithms for data preprocessing, model building, and evaluation.\n",
    "2. linear_model:\n",
    "\n",
    "A sub-module within scikit-learn that contains various algorithms for linear models.\n",
    "These models learn a linear relationship between features and a target variable.\n",
    "3. LogisticRegression:\n",
    "\n",
    "A specific classification algorithm from the linear_model sub-module.\n",
    "Used for problems where the target variable can have discrete categories (e.g., spam or not spam, passed or failed).\n",
    "It estimates the probability of a data point belonging to a particular category.\n",
    "In simpler terms:\n",
    "\n",
    "Imagine you have a dataset where each data point has several features (like age, income, etc.) and you want to predict whether it belongs to a specific category (like creditworthy or not). Logistic Regression helps you learn a linear relationship between the features and the category labels. It then uses this relationship to estimate the probability of a new data point falling into each category.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Logistic Regression is commonly used in various classification tasks:\n",
    "Spam filtering\n",
    "Email categorization\n",
    "Customer churn prediction\n",
    "Fraud detection\n",
    "Image classification (for binary problems)\n",
    "Here are some additional points to consider:\n",
    "\n",
    "Logistic Regression is a powerful and interpretable model, meaning it's relatively easy to understand the factors influencing the predictions.\n",
    "However, it might not always be the most accurate model for complex problems.\n",
    "Scikit-learn offers various parameters to tune the Logistic Regression model for better performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "what it is used for in ML\n",
    "\n",
    "The code you provided imports three important functions from the sklearn.metrics module in Python, commonly used for evaluating the performance of machine learning models, especially in classification tasks. Here's a breakdown of what each function does:\n",
    "\n",
    "1. accuracy_score:\n",
    "\n",
    "Calculates the overall accuracy of a classification model.\n",
    "Represents the proportion of predictions that were correct.\n",
    "A common metric, but it can be misleading in imbalanced datasets.\n",
    "2. confusion_matrix:\n",
    "\n",
    "Creates a table that summarizes the predictions of a classification model.\n",
    "Rows represent the actual classes, and columns represent the predicted classes.\n",
    "Each cell shows the number of instances that were actually in one class but predicted to be in another.\n",
    "Useful for visualizing how the model is performing on different classes.\n",
    "3. classification_report:\n",
    "\n",
    "Generates a report that provides various metrics for evaluating the performance of a classification model.\n",
    "Includes metrics like precision, recall, F1-score, and support for each class.\n",
    "Provides a more comprehensive view of the model's performance beyond just accuracy.\n",
    "In essence, these functions help you:\n",
    "\n",
    "Assess how well your model generalizes to unseen data.\n",
    "Identify areas where the model might be struggling with specific classes.\n",
    "Compare the performance of different models on the same task.\n",
    "By using these metrics, you can gain valuable insights into the strengths and weaknesses of your machine learning models and make informed decisions about how to improve their performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold What it is used for and what it does in ML?\n",
    "\n",
    "Certainly! The code from sklearn.model_selection import KFold imports a class for implementing K-Fold cross-validation, a technique commonly used in machine learning to evaluate the performance of models.\n",
    "\n",
    "Here's how KFold cross-validation works:\n",
    "\n",
    "Splits the data: The data is divided into k folds (usually equal in size).\n",
    "Iteratively trains and tests:\n",
    "In each iteration, one fold is used as the test set (data to be predicted on), and the remaining k-1 folds are combined to form the training set (data used to train the model).\n",
    "The model is trained on the training set, and its performance is evaluated on the test set.\n",
    "Aggregates performance: The performance scores (e.g., accuracy) from each iteration are averaged to get a more robust estimate of the model's generalizability.\n",
    "Benefits of using KFold cross-validation:\n",
    "\n",
    "Reduces overfitting: By training on different subsets of the data, KFold helps prevent the model from memorizing the training data and improving its ability to generalize to unseen data.\n",
    "Provides a more reliable estimate of model performance: Averaging the performance scores across multiple folds reduces the variance and provides a more accurate idea of how well the model will perform on new data.\n",
    "Key points to remember:\n",
    "\n",
    "The number of folds (k) is a hyperparameter that can be tuned to optimize the evaluation process.\n",
    "KFold is particularly useful when the dataset is relatively small, as it ensures that all data points are used for both training and testing.\n",
    "In summary, KFold cross-validation is a valuable tool for machine learning practitioners to:\n",
    "\n",
    "Obtain a more objective assessment of model performance.\n",
    "Reduce the risk of overfitting and improve thegeneralizability of models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV What it is and what it does?\n",
    "\n",
    "The code from sklearn.model_selection import GridSearchCV imports a powerful tool from scikit-learn: GridSearchCV. It's used for hyperparameter tuning in machine learning models.\n",
    "\n",
    "Hyperparameters are settings that control the learning process of a model. They are not directly learned from the data but are set by the user before training. Choosing the right hyperparameters can significantly impact the performance of your model.\n",
    "\n",
    "GridSearchCV automates hyperparameter tuning:\n",
    "\n",
    "Defines a search space: You specify a grid of values for each hyperparameter you want to tune.\n",
    "Trains the model on all combinations: GridSearchCV systematically trains the model on every combination of hyperparameter values from the defined grid.\n",
    "Evaluates performance: For each combination, it evaluates the model's performance using a scoring metric (e.g., accuracy).\n",
    "Identifies the best combination: GridSearchCV selects the combination of hyperparameters that results in the best performance according to the scoring metric.\n",
    "Benefits of using GridSearchCV:\n",
    "\n",
    "Saves time and effort: Automates the process of trying out different hyperparameter combinations, which can be tedious and time-consuming.\n",
    "Improves model performance: Helps you find the hyperparameters that lead to the best possible performance for your model.\n",
    "Provides insights: Offers a systematic approach to understanding how different hyperparameters affect your model's behavior.\n",
    "Things to consider:\n",
    "\n",
    "GridSearchCV can be computationally expensive, especially for large datasets or complex models.\n",
    "It's essential to choose an appropriate scoring metric that reflects your objective.\n",
    "GridSearchCV only explores the defined search space. It might miss even better hyperparameter combinations outside the grid.\n",
    "In essence, GridSearchCV is a valuable tool for:\n",
    "\n",
    "Finding the optimal hyperparameters for your machine learning model.\n",
    "Systematically improving the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an array of numbers from -2 to 2 (excluding 3)\n",
    "result = np.arange(-2, 3)\n",
    "\n",
    "# Print the array\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]) it is giving me this values for 10.0 ** np.arange(-2, 3)\n",
    "\n",
    "You're absolutely right! The output array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]) is indeed the correct result for 10.0 ** np.arange(-2, 3).\n",
    "\n",
    "Here's a breakdown of how it works:\n",
    "\n",
    "np.arange(-2, 3): This part generates an array of numbers from -2 (inclusive) to 3 (exclusive). In this case, the array will be [-2, -1, 0, 1, 2].\n",
    "\n",
    "10.0 **: This applies the exponent operation (**) to each element in the array. The exponent is 10.0.\n",
    "\n",
    "Calculation:\n",
    "\n",
    "10.0 ** -2 = 0.01 (10 raised to the power of -2 is 1 divided by 100)\n",
    "10.0 ** -1 = 0.1 (10 raised to the power of -1 is 1 divided by 10)\n",
    "10.0 ** 0 = 1.0 (10 raised to the power of 0 is 1)\n",
    "10.0 ** 1 = 10.0 (10 raised to the power of 1 is 10)\n",
    "10.0 ** 2 = 100.0 (10 raised to the power of 2 is 10 multiplied by 10)\n",
    "Result: The final array becomes [0.01, 0.1, 1.0, 10.0, 100.0].\n",
    "\n",
    "The scientific notation (1.e-02, 1.e-01, etc.) is a way to represent very small or large numbers in a more compact and readable format. For example, 1.e-02 is equivalent to 1 * 10^(-2), which is 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.0 **np.arange(-2,3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grid={'C':10.0 **np.arange(-2,3),'penalty':['l1','l2']}\n",
    "The code you provided, grid={'C':10.0 **np.arange(-2,3),'penalty':['l1','l2']}, defines a dictionary that is likely used for hyperparameter tuning in machine learning, specifically for a model like Logistic Regression. Here's a breakdown of what it means:\n",
    "\n",
    "1. Dictionary:\n",
    "\n",
    "This creates a collection of key-value pairs to store parameters for the model.\n",
    "2. Key 'C':\n",
    "\n",
    "Represents the regularization parameter in Logistic Regression.\n",
    "It controls the trade-off between model complexity andgeneralizability.\n",
    "Higher 'C' values lead to simpler models that might underfit, while lower values can result in overfitting.\n",
    "3. Value for 'C':\n",
    "\n",
    "This is not a single value but an array of values generated by 10.0 ** np.arange(-2, 3).\n",
    "np.arange(-2, 3) creates an array [-2, -1, 0, 1, 2].\n",
    "Raising each element to the power of 10.0 using ** produces the array [100, 10, 1, 0.1, 0.01].\n",
    "4. Key 'penalty':\n",
    "\n",
    "Represents the type of regularization penalty used.\n",
    "5. Value for 'penalty':\n",
    "\n",
    "This is a list containing two options: ['l1', 'l2'].\n",
    "'l1' (L1 regularization) encourages sparsity by shrinking some coefficients to zero.\n",
    "'l2' (L2 regularization) penalizes large coefficients, making the model smoother.\n",
    "In essence:\n",
    "\n",
    "This dictionary defines a grid of hyperparameter values to be explored during training. The model will be trained with five different 'C' values (from the array) and both L1 and L2 penalties. This allows you to evaluate the impact of different regularization settings on the model's performance.\n",
    "\n",
    "Common Use Case:\n",
    "\n",
    "This is often used with GridSearchCV in scikit-learn, a tool that automates hyperparameter tuning. GridSearchCV will systematically try out all combinations of parameters in the grid and select the one that yields the best performance based on a chosen metric (e.g., accuracy).\n",
    "\n",
    "Benefits:\n",
    "\n",
    "Efficiently explores multiple hyperparameter settings.\n",
    "Helps identify the optimal configuration for your model.\n",
    "Reduces the risk of overfitting or underfitting."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Im not understanding about regularization\n",
    "Regularization is a crucial concept in machine learning that helps prevent a common problem called overfitting. Here's an explanation to improve your understanding:\n",
    "\n",
    "Imagine you're training a model to predict house prices.\n",
    "\n",
    "Without regularization: The model might become too focused on capturing every tiny detail in the training data, even if those details are just noise or irrelevant information. This can lead to the model performing well on the training data but failing to generalize to unseen data (poor performance on new houses). This is called overfitting.\n",
    "Regularization techniques act like a leash, preventing the model from becoming overly complex and adapting too closely to the training data. Here are some common approaches:\n",
    "\n",
    "L1 regularization (Lasso): Shrinks the weights of some features towards zero, effectively removing them from the model. This encourages sparsity and reduces model complexity.\n",
    "\n",
    "L2 regularization (Ridge): Penalizes large weights, making the model smoother and less likely to overfit.\n",
    "\n",
    "Data augmentation: Artificially creates new data points by modifying existing ones, increasing the diversity of the training data and making the model more robust.\n",
    "\n",
    "Benefits of regularization:\n",
    "\n",
    "Reduces overfitting: Improves the model's ability to generalize to unseen data.\n",
    "Prevents overconfidence: Regularized models are less likely to be overly certain about their predictions.\n",
    "Improves model interpretability: By reducing the number of features or shrinking their weights, regularization can make it easier to understand how the model makes decisions.\n",
    "Choosing the right regularization technique and its strength (e.g., the value of C) is important. Experimenting with different approaches can help you find the best configuration for your specific problem.\n",
    "\n",
    "Here are some additional points to consider:\n",
    "\n",
    "Regularization is a balancing act. Too much regularization can lead to underfitting, where the model is too simple to capture the underlying patterns in the data.\n",
    "The effectiveness of different regularization techniques can vary depending on the type of machine learning problem and the characteristics of your data.\n",
    "By understanding regularization and applying it effectively, you can improve thegeneralizability and performance of your machine learning models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What sparsity in L1 \n",
    "L1 regularization, also known as Lasso regression, is known for its ability to promote sparsity in models. Here's what that means:\n",
    "\n",
    "Sparsity refers to the condition where many of the coefficients (weights) associated with features in a model become zero. In simpler terms, the model effectively ignores some features and relies only on a subset of them for making predictions.\n",
    "\n",
    "Why is sparsity desirable?\n",
    "\n",
    "Reduces overfitting: By eliminating unnecessary features, L1 regularization helps prevent the model from memorizing noise or irrelevant information in the training data. This leads to a more generalizable model that performs well on unseen data.\n",
    "Improves model interpretability: With fewer features influencing the model's predictions, it becomes easier to understand which features are most important and how they contribute to the final output.\n",
    "Feature selection: L1 regularization can act as a feature selection technique, automatically identifying the most relevant features for the task.\n",
    "How does L1 regularization achieve sparsity?\n",
    "\n",
    "L1 regularization works by adding a penalty term to the model's objective function. This penalty term is based on the absolute value of the coefficients. As the model is trained, the algorithm tries to minimize both the error on the training data and the sum of the absolute values of the coefficients.\n",
    "\n",
    "Here's the key: Minimizing the absolute values of the coefficients often leads to some of them becoming exactly zero. This is because reducing a coefficient to zero eliminates its contribution to the penalty term entirely, while reducing its value further only has a diminishing effect.\n",
    "\n",
    "L1 vs. L2 Regularization:\n",
    "\n",
    "L2 regularization (Ridge regression): Penalizes the squared values of the coefficients, which tends to shrink them towards zero but rarely drives them to exactly zero. This results in a smoother model with all features still contributing, but with less weight.\n",
    "L1 regularization: More aggressively pushes coefficients to zero, leading to sparser models with fewer features playing a role.\n",
    "Choosing between L1 and L2 depends on your specific problem:\n",
    "\n",
    "If interpretability and feature selection are important, L1 might be preferable.\n",
    "If reducing overfitting is the primary concern, L2 can be effective as well.\n",
    "In conclusion, L1 regularization promotes sparsity by penalizing the absolute values of coefficients, driving some of them to zero and creating a simpler, more interpretable model that is less prone to overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "penalizes large coefficients in L2 what it is mean penalize in this context?\n",
    "\n",
    "In the context of L2 regularization (Ridge regression), \"penalizes large coefficients\" means that the model is discouraged from having coefficients with very high absolute values. This is achieved by adding a term to the model's objective function that increases as the sum of the squared coefficients grows.\n",
    "\n",
    "Here's a breakdown of the concept:\n",
    "\n",
    "1. Coefficients: These represent the weights assigned to each feature in the model. They determine how much influence each feature has on the final prediction.\n",
    "\n",
    "2. Large Coefficients: When a coefficient has a large absolute value (positive or negative), it signifies that the corresponding feature has a strong impact on the model's predictions.\n",
    "\n",
    "3. Penalization: L2 regularization introduces a penalty term that is proportional to the sum of the squared coefficients. This means that the model is \"punished\" for having coefficients with large magnitudes.\n",
    "\n",
    "Why Penalize Large Coefficients?\n",
    "\n",
    "Reduces Overfitting: By discouraging overly influential features, L2 regularization prevents the model from becoming too reliant on specific details in the training data that might not generalize well to unseen data.\n",
    "Improves Model Stability: Large coefficients can make the model sensitive to small changes in the data. Penalizing them leads to a more stable model that is less likely to produce significantly different outputs for slightly different inputs.\n",
    "How Does Penalization Work?\n",
    "\n",
    "Imagine training a model with two features: F1 and F2.\n",
    "\n",
    "Scenario 1: The model assigns a large positive coefficient to F1 and a small coefficient to F2. This means that the model heavily relies on F1 for predictions.\n",
    "Scenario 2: With L2 regularization, the penalty term due to the large coefficient of F1 becomes significant. To minimize the overall objective function (including the penalty), the model might slightly adjust the coefficients. It could:\n",
    "Decrease the coefficient of F1 to reduce its influence.\n",
    "Increase the coefficient of F2 to compensate for the decrease in F1's impact.\n",
    "This process encourages the model to distribute the influence more evenly among features, avoiding overreliance on any single one.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "L2 regularization doesn't necessarily drive coefficients to zero, unlike L1 regularization (Lasso) which promotes sparsity.\n",
    "The strength of the penalty is controlled by a hyperparameter called \"lambda\". A higher lambda value leads to stronger penalization and smaller coefficients.\n",
    "L2 regularization is a widely used technique to improve thegeneralizability and stability of machine learning models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Explain with an example of datasetr\n",
    "Absolutely, let's use an example to understand how L2 regularization penalizes large coefficients:\n",
    "\n",
    "Imagine you're building a model to predict housing prices based on several features:\n",
    "\n",
    "Features: Square footage (sqft), number of bedrooms (bedrooms), neighborhood quality (quality)\n",
    "Target Variable: Selling price (price)\n",
    "Without L2 regularization:\n",
    "\n",
    "The model might learn that square footage (sqft) has the strongest influence on price, assigning a very high coefficient to this feature.\n",
    "Other features, like bedrooms and quality, might have smaller coefficients, implying less impact on the predicted price.\n",
    "This can lead to overfitting:\n",
    "\n",
    "The model becomes overly reliant on sqft, potentially capturing noise or irrelevant details specific to the training data.\n",
    "If a new house has a slightly different sqft value than those in the training data, the prediction might be significantly off.\n",
    "L2 regularization comes to the rescue:\n",
    "\n",
    "It adds a penalty term to the model's objective function that increases with the sum of the squared coefficients.\n",
    "During training, the model tries to minimize both the error on the training data and the penalty term.\n",
    "Impact on coefficients:\n",
    "\n",
    "The high coefficient of sqft is penalized heavily due to its large squared value.\n",
    "To reduce the penalty, the model might slightly decrease the coefficient of sqft.\n",
    "Coefficients of other features (bedrooms, quality) might be slightly increased to compensate for the decrease in sqft's influence.\n",
    "Benefits:\n",
    "\n",
    "The model becomes less reliant on any single feature, leading to bettergeneralizability.\n",
    "It's less likely to overfit to specific details in the training data.\n",
    "Important to note:\n",
    "\n",
    "L2 regularization doesn't necessarily eliminate features entirely, unlike L1 which promotes sparsity.\n",
    "The strength of the penalty is controlled by a hyperparameter that needs to be tuned for optimal performance.\n",
    "In essence, L2 regularization acts like a damper, preventing the model from giving excessive weight to any single feature and promoting a more balanced andgeneralizable model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For this what L1 does\n",
    "L1 regularization, also known as Lasso regression, works differently from L2 regularization in how it penalizes coefficients. Here's a breakdown of what L1 does:\n",
    "\n",
    "Sparsity:\n",
    "\n",
    "L1 aims to achieve sparsity, meaning it drives some of the coefficients in the model to become exactly zero.\n",
    "This effectively removes those features from the model, making it rely only on a subset of features for predictions.\n",
    "Penalizing Large Coefficients:\n",
    "\n",
    "While it might seem counterintuitive, L1 achieves sparsity by penalizing the absolute values of the coefficients, not their squares like L2.\n",
    "As the model is trained, it minimizes the sum of absolute coefficient values along with the prediction error.\n",
    "Why Does This Lead to Sparsity?\n",
    "\n",
    "Reducing a coefficient's absolute value towards zero has a diminishing effect on the penalty term.\n",
    "Completely eliminating a feature (setting its coefficient to zero) removes its contribution to the penalty altogether.\n",
    "Therefore, the model often finds it more efficient to drive certain coefficients to zero rather than continuously shrinking them towards zero.\n",
    "Benefits of Sparsity:\n",
    "\n",
    "Reduced Overfitting: By eliminating irrelevant features, L1 helps prevent the model from memorizing noise or unimportant details in the data.\n",
    "Improved Interpretability: With fewer features influencing predictions, it's easier to understand which features are most crucial for the model's decisions.\n",
    "Feature Selection: L1 can act as a feature selection technique, automatically identifying the most relevant features for the task.\n",
    "Comparison with L2:\n",
    "\n",
    "L2 regularization (Ridge regression) shrinks coefficients towards zero but rarely drives them to zero, resulting in a smoother model with all features still contributing.\n",
    "L1 is more aggressive, forcing some coefficients to zero and creating sparser models.\n",
    "Choosing Between L1 and L2:\n",
    "\n",
    "If interpretability and feature selection are important, L1 might be preferred.\n",
    "If reducing overfitting is the primary concern, L2 can also be effective.\n",
    "In conclusion, L1 regularization penalizes the absolute values of coefficients, achieving sparsity by driving some of them to zero. This leads to a simpler, more interpretable model that is less prone to overfitting and can help identify the most important features for the task."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Give me an example for L1 with house price\n",
    "\n",
    "L1 Regularization Example: Predicting House Prices\n",
    "Scenario: You're building a model to predict house prices based on several features:\n",
    "\n",
    "Features: Square Footage (sqft), Number of Bedrooms (bedrooms), Lot Size (lot_size), Garage Size (garage_size), Neighborhood Quality (quality)\n",
    "Target Variable: Selling Price (price)\n",
    "Without L1 Regularization:\n",
    "\n",
    "The model might assign high coefficients to features that seem strongly correlated with price, like sqft and lot_size.\n",
    "Less influential features, like garage_size or quality, might have lower coefficients.\n",
    "Potential Issues:\n",
    "\n",
    "The model might overfit to the training data, becoming too reliant on specific features that may not generalize well to unseen houses.\n",
    "L1 Regularization in Action:\n",
    "\n",
    "During training, L1 penalizes the absolute values of the coefficients.\n",
    "To minimize the penalty, the model might:\n",
    "Drive coefficients of less important features (e.g., garage_size) to zero, effectively removing them from the model.\n",
    "Slightly adjust the coefficients of remaining features (sqft, lot_size) to maintain accuracy.\n",
    "Benefits:\n",
    "\n",
    "The model becomes more focused on the most relevant features for predicting price.\n",
    "It's less likely to overfit to noise or irrelevant details in the training data.\n",
    "You gain insights into which features are most important for house price prediction.\n",
    "Important Note:\n",
    "\n",
    "The specific features removed by L1 will depend on the data and the strength of the regularization penalty.\n",
    "Additional Considerations:\n",
    "\n",
    "L1 regularization might not be the best choice if you need to understand the impact of all features on price.\n",
    "Tuning the L1 penalty parameter is crucial to find the right balance between sparsity and model performance.\n",
    "In essence, L1 regularization helps you build a simpler, more interpretable model for house price prediction by focusing on the most important features and reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grid={'C':10.0 **np.arange(-2,3),'penalty':['l1','l2']} How many parameters will have for grid\n",
    "The grid you defined has a total of 7 parameters for hyperparameter tuning. Here's the breakdown:\n",
    "\n",
    "1. Key 'C':\n",
    "\n",
    "Uses np.arange(-2, 3) which creates an array [-2, -1, 0, 1, 2].\n",
    "Raises each element to the power of 10.0, resulting in the array [100, 10, 1, 0.1, 0.01].\n",
    "This defines 5 different values for the regularization parameter 'C'.\n",
    "2. Key 'penalty':\n",
    "\n",
    "This is a list containing 2 options: ['l1', 'l2'].\n",
    "Total Parameters:\n",
    "\n",
    "Since there are 5 values for 'C' and 2 options for 'penalty', the total number of parameter combinations explored by the grid is:\n",
    "\n",
    "Number of parameters = Number of values in 'C' * Number of options in 'penalty'\n",
    "                       = 5 * 2\n",
    "                       = 7\n",
    "Therefore, the grid you defined will evaluate the model's performance with 7 different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grid={'C':10.0 **np.arange(-2,3),'penalty':['l1','l2']} How many parameters will have for grid\n",
    "The grid you defined has a total of 7 parameters for hyperparameter tuning. Here's the breakdown:\n",
    "\n",
    "1. Key 'C':\n",
    "\n",
    "Uses np.arange(-2, 3) which creates an array [-2, -1, 0, 1, 2].\n",
    "Raises each element to the power of 10.0, resulting in the array [100, 10, 1, 0.1, 0.01].\n",
    "This defines 5 different values for the regularization parameter 'C'.\n",
    "2. Key 'penalty':\n",
    "\n",
    "This is a list containing 2 options: ['l1', 'l2'].\n",
    "Total Parameters:\n",
    "\n",
    "Since there are 5 values for 'C' and 2 options for 'penalty', the total number of parameter combinations explored by the grid is:\n",
    "\n",
    "Number of parameters = Number of values in 'C' * Number of options in 'penalty'\n",
    "                       = 5 * 2\n",
    "                       = 7\n",
    "Therefore, the grid you defined will evaluate the model's performance with 7 different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By using different 7 hyperparameter combinations. What it does and what it is useful\n",
    "The grid you defined (grid={'C':10.0 **np.arange(-2,3),'penalty':['l1','l2']}) with 7 hyperparameter combinations is likely used for hyperparameter tuning in machine learning. Here's what it does and why it's useful:\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "Machine learning models often have parameters that control their learning process but are not directly learned from the data. These are called hyperparameters.\n",
    "Choosing the right hyperparameters is crucial for achieving optimal model performance.\n",
    "What the Grid Does:\n",
    "\n",
    "Defines a Search Space: Your grid specifies different values to explore for two hyperparameters:\n",
    "Regularization parameter (C): Controls the trade-off between model complexity and overfitting.\n",
    "Penalty type (penalty): L1 or L2 regularization, influencing how the model handles coefficients.\n",
    "Explores Combinations: It creates 7 different combinations of these hyperparameter values (5 values of C and 2 penalty types).\n",
    "Trains the Model: For each combination, the model is trained on the data.\n",
    "Evaluates Performance: A scoring metric (e.g., accuracy) is used to assess the performance of the model with each hyperparameter combination.\n",
    "Benefits of Using a Grid:\n",
    "\n",
    "Systematic Approach: Ensures that a variety of hyperparameter settings are explored, reducing the risk of missing good options.\n",
    "Identifies Optimal Settings: Helps find the combination of hyperparameters that leads to the best performance for your specific model and data.\n",
    "Reduces Manual Effort: Automates the process of trying out different hyperparameter values, saving you time and resources.\n",
    "How it's Useful:\n",
    "\n",
    "Improves Model Generalizability: By finding the best hyperparameter settings, you can help prevent the model from overfitting and performing well on unseen data.\n",
    "Enhances Model Performance: Tuning hyperparameters can significantly improve the accuracy, precision, recall, or other relevant metrics for your machine learning task.\n",
    "Provides Insights: The results of the grid search can offer valuable insights into how different hyperparameters affect the model's behavior.\n",
    "In essence, using a grid search with multiple hyperparameter combinations is a powerful technique to optimize your machine learning models and achieve better results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So finally it will one value from C and one from Penalty right which is good for the ML model?\n",
    "\n",
    "You're absolutely right. The ultimate goal of hyperparameter tuning using a grid search is to identify the single best combination of hyperparameter values that leads to the best performance for your machine learning model.\n",
    "\n",
    "The grid you defined explores 7 different combinations:\n",
    "\n",
    "5 values for the regularization parameter (C)\n",
    "2 options for the penalty type (L1 or L2)\n",
    "After evaluating all 7 combinations:\n",
    "\n",
    "A specific scoring metric (e.g., accuracy, F1-score) is used to compare their performance.\n",
    "The combination that yields the highest score is considered the optimal choice for your model.\n",
    "Important Points:\n",
    "\n",
    "The best hyperparameter combination can vary depending on the specific dataset, model architecture, and chosen metric.\n",
    "There's no guarantee that the optimal values will be at the extremes of the grid you defined.\n",
    "More sophisticated hyperparameter tuning techniques can explore a wider range of values or use more fine-grained search methods.\n",
    "In conclusion, while the grid search explores multiple combinations, it ultimately aims to find the single best value for C and the most suitable penalty type that optimizes your machine learning model's performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cv=KFold(n_splits=5,random_state=None,shuffle=False) What it does and what it is useful in ML?\n",
    "\n",
    "The code cv=KFold(n_splits=5, random_state=None, shuffle=False) defines a cross-validation strategy commonly used in machine learning. Here's a breakdown of what it does and its usefulness:\n",
    "\n",
    "What it Does:\n",
    "\n",
    "KFold is a specific type of cross-validation technique.\n",
    "It splits the data into k folds (groups) of approximately equal size.\n",
    "In your case, n_splits=5 specifies that the data will be divided into 5 folds.\n",
    "How it Works:\n",
    "\n",
    "Random Split (Optional): By default (shuffle=False), KFold maintains the original order of the data. If shuffle=True is set, the data is shuffled before splitting.\n",
    "Fold Selection: In each iteration:\n",
    "One fold is used as the validation set for testing the model's performance.\n",
    "The remaining folds are combined to form the training set used to train the model.\n",
    "Model Training & Evaluation:\n",
    "The model is trained on the training set.\n",
    "Its performance is evaluated on the held-out validation set using a chosen metric (e.g., accuracy, F1-score).\n",
    "Iteration: This process is repeated k times, using each fold for validation exactly once.\n",
    "Benefits of Cross-Validation:\n",
    "\n",
    "Reduces Overfitting: By evaluating the model on unseen data (validation set) in each iteration, KFold helps prevent the model from memorizing noise or specific patterns in the training data.\n",
    "Provides More Reliable Performance Estimates: The performance score obtained by averaging the scores from all folds is a more robust estimate of how well the model will generalize to unseen data.\n",
    "Why is it Useful?\n",
    "\n",
    "KFold is a simple and effective cross-validation technique.\n",
    "It's often used as a baseline for more advanced cross-validation methods.\n",
    "It helps ensure that the reported performance of your model is not overly optimistic and reflects itsgeneralizability.\n",
    "Additional Considerations:\n",
    "\n",
    "The choice of n_splits (number of folds) can impact the variance of the performance estimate. More folds generally lead to lower variance but require more computation time.\n",
    "KFold assumes that the data is independent and identically distributed (i.i.d.), which may not always be the case.\n",
    "In essence, KFold is a valuable tool for evaluating thegeneralizability of your machine learning models and reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_class=LogisticRegression()\n",
    "grid={'C':10.0 **np.arange(-2,3),'penalty':['l1','l2']}\n",
    "cv=KFold(n_splits=5,random_state=None,shuffle=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Splitting the Data:\n",
    "\n",
    "Python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "Use code with caution.\n",
    "X and y: These represent your data. Typically, X is your feature matrix containing the independent variables, and y is the target vector containing the dependent variables (what you want to predict).\n",
    "train_test_split(X, y, train_size=0.7): This function splits the data into training and testing sets based on the specified train_size parameter.\n",
    "train_size (default=0.7): This defines the proportion of data to be used for the training set. In this case, 70% of the data will be allocated to the training set (X_train and y_train).\n",
    "X_train: This variable stores the training data features.\n",
    "X_test: This variable stores the testing data features.\n",
    "y_train: This variable stores the training data target labels.\n",
    "y_test: This variable stores the testing data target labels.\n",
    "In essence, this code snippet splits your data into two sets:\n",
    "\n",
    "Training set (70%) used to train the machine learning model.\n",
    "Testing set (30%) used to evaluate the model's performance on unseen data. This helps assess how well the model generalizes to new examples.\n",
    "Important Note: Splitting data into training and testing sets is a crucial step in machine learning to prevent overfitting and ensure the model can learn from the data without memorizing specific patterns."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After predicting the values after that the values are stored in y_test right?\n",
    "\n",
    "No, y_test is not where the predicted values are stored. Here's the clarification:\n",
    "\n",
    "1. Splitting the Data:\n",
    "\n",
    "train_test_split divides your data into training and testing sets.\n",
    "X_train and y_train represent the features and target variables for training the model.\n",
    "X_test and y_test represent the features and target variables for testing, which the model hasn't seen before.\n",
    "2. Training the Model:\n",
    "\n",
    "You use the training data (X_train and y_train) to train your machine learning model.\n",
    "The model learns the relationships between the features and the target variable.\n",
    "3. Making Predictions:\n",
    "\n",
    "Once trained, you use the model to predict the target variable for unseen data.\n",
    "This unseen data can be either from a completely new dataset or the testing set (X_test).\n",
    "4. Comparing Predictions with Actual Values:\n",
    "\n",
    "The predicted values for the unseen data are stored in a separate variable, typically named y_pred.\n",
    "y_test contains the actual target values for the testing data.\n",
    "5. Evaluation:\n",
    "\n",
    "You compare the predicted values (y_pred) with the actual values (y_test) using evaluation metrics like accuracy, precision, recall, or F1-score.\n",
    "This helps you assess how well the model performs on unseen data.\n",
    "In summary:\n",
    "\n",
    "y_test stores the true target values for the testing set, which the model is trying to predict.\n",
    "Your model's predictions are stored in a separate variable (y_pred), and you compare them with y_test to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "During testing the y-test will be hidden for the Machine learned model right?\n",
    "\n",
    "You're absolutely right. During testing, the target variable in the testing set (y_test) is indeed hidden from the machine learning model. Here's why:\n",
    "\n",
    "The Goal of Testing:\n",
    "\n",
    "The primary purpose of testing is to evaluate how well the model generalizes to unseen data.\n",
    "If the model were allowed to see the target values (y_test) during testing, it could potentially memorize them and achieve an unrealistically high performance score.\n",
    "Ensuring Generalizability:\n",
    "\n",
    "By hiding y_test, we force the model to rely solely on its learned patterns from the training data to make predictions.\n",
    "This allows us to assess its ability to apply those patterns to new, unseen examples.\n",
    "Evaluation Process:\n",
    "\n",
    "Training: The model learns from the training data (X_train and y_train).\n",
    "Testing: The model makes predictions for the unseen testing data (X_test).\n",
    "Comparison: The predicted values (y_pred) are compared with the hidden actual values (y_test).\n",
    "Evaluation Metrics: Performance metrics like accuracy, precision, recall, or F1-score are calculated based on this comparison.\n",
    "Maintaining a Blind Spot:\n",
    "\n",
    "Hiding y_test essentially creates a blind spot for the model during testing.\n",
    "It can only rely on its internal knowledge to make predictions, mimicking a real-world scenario where the true target values are unknown.\n",
    "Importance of Hidden Testing Data:\n",
    "\n",
    "This approach helps prevent overfitting and ensures a more objective assessment of the model'sgeneralizability.\n",
    "It allows you to determine how well the model performs on data it hasn't seen before, which is crucial for real-world applications.\n",
    "In conclusion, hiding the target variable in the testing set is a fundamental principle in machine learning to ensure that the model's performance reflects its true ability to learn and generalize from the data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So finally will they check accuracy between y_predict and y_test right?\n",
    "\n",
    "Yes, exactly! After the model makes predictions on the testing data, the accuracy is indeed calculated by comparing the predicted values (y_pred) with the actual target values (y_test).\n",
    "\n",
    "Here's a breakdown of the process:\n",
    "\n",
    "Training: The model learns from the training data (X_train and y_train).\n",
    "Testing: The trained model makes predictions for the unseen testing data (X_test). These predictions are stored in y_pred.\n",
    "Evaluation: The actual target values for the testing data are retrieved from y_test.\n",
    "Accuracy Calculation: A metric like accuracy is calculated by comparing the elements in y_pred with the corresponding elements in y_test. Here's a common way to do it:\n",
    "Python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "Use code with caution.\n",
    "This code snippet calculates the accuracy as the proportion of correct predictions made by the model on the testing data.\n",
    "\n",
    "Why is this Important?\n",
    "\n",
    "The accuracy score obtained from comparing y_pred and y_test reflects how well the model generalizes to unseen data.\n",
    "It helps you assess whether the model has learned meaningful patterns from the training data that can be applied to new examples.\n",
    "Other Evaluation Metrics:\n",
    "\n",
    "Accuracy is just one way to evaluate model performance. Other metrics like precision, recall, F1-score, or ROC AUC may be more suitable depending on the specific task and the nature of your data.\n",
    "In conclusion, comparing y_pred with y_test is a crucial step in machine learning to evaluate the model'sgeneralizability and effectiveness."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If the accuracy is high then it is good predicted right?\n",
    "\n",
    "High accuracy can be a good indicator of a well-performing model, but it's not always the whole story. Here's why:\n",
    "\n",
    "Accuracy simply measures the proportion of correct predictions made by the model. While a high accuracy is desirable, it doesn't necessarily guarantee a good model in all situations.\n",
    "\n",
    "Here are some limitations of relying solely on accuracy:\n",
    "\n",
    "Class Imbalance: If your data has imbalanced classes (one class has significantly more examples than others), a model might achieve high accuracy by simply predicting the majority class most of the time. This can be misleading if the model performs poorly on the minority class.\n",
    "Data Leakage: If there's any overlap between the training and testing data (e.g., due to data leakage), the model might memorize specific patterns that don't generalize well to unseen data, leading to artificially high accuracy.\n",
    "Therefore, it's important to consider other factors alongside accuracy when evaluating a model's performance:\n",
    "\n",
    "Precision and Recall: These metrics measure how well the model identifies true positives and avoids false positives/negatives, which can be crucial depending on the task.\n",
    "ROC AUC: This metric evaluates the model's ability to distinguish between positive and negative cases.\n",
    "F1-Score: This metric combines precision and recall into a single score, useful for imbalanced datasets.\n",
    "Domain Knowledge: Understanding the problem you're trying to solve and the expected behavior of the model is essential for interpreting its performance.\n",
    "Here's a summary:\n",
    "\n",
    "High accuracy is a positive sign, but it should be interpreted in conjunction with other metrics and domain knowledge.\n",
    "Choose appropriate evaluation metrics based on your specific task and data characteristics.\n",
    "Consider factors like class imbalance and data leakage that can affect accuracy.\n",
    "In conclusion, while accuracy is a valuable metric, a comprehensive evaluation using multiple metrics and domain expertise is essential to determine if a model's predictions are truly good and generalizable."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Independent and Dependent Features\n",
    "X=df.drop(\"Class\",axis=1)\n",
    "y=df.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Amount  \n",
       "0       149.62  \n",
       "1         2.69  \n",
       "2       378.66  \n",
       "3       123.50  \n",
       "4        69.99  \n",
       "...        ...  \n",
       "284802    0.77  \n",
       "284803   24.79  \n",
       "284804   67.88  \n",
       "284805   10.00  \n",
       "284806  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf=GridSearchCV(log_class,grid,cv=cv,n_jobs=-1,scoring='f1_macro') What it does and how it is useful\n",
    "\n",
    "The code clf = GridSearchCV(log_class, grid, cv=cv, n_jobs=-1, scoring='f1_macro') defines a powerful tool for hyperparameter tuning in machine learning using scikit-learn. Here's a breakdown of what it does and how it's useful:\n",
    "\n",
    "1. Components:\n",
    "\n",
    "GridSearchCV: This class from scikit-learn automates the process of hyperparameter tuning.\n",
    "log_class: This is likely a classifier object (e.g., LogisticRegression) you want to tune.\n",
    "grid: This is a dictionary that defines the search space for hyperparameter values.\n",
    "cv: This defines the cross-validation strategy (here, cv is likely a KFold object).\n",
    "n_jobs=-1: This utilizes all available cores on your machine for parallel processing (if applicable).\n",
    "scoring='f1_macro': This specifies the metric used to evaluate model performance (F1-score with macro averaging).\n",
    "2. What it Does:\n",
    "\n",
    "Explores Hyperparameter Combinations: The grid dictionary defines different values to try for various hyperparameters of the classifier (log_class).\n",
    "Cross-Validation: It uses the specified cross-validation strategy (cv) to split the data into multiple folds.\n",
    "Trains and Evaluates: For each combination of hyperparameter values:\n",
    "The classifier is trained on a subset of the data (folds).\n",
    "Its performance is evaluated on the remaining fold using the scoring metric (F1-macro in this case).\n",
    "Finds the Best: It identifies the combination of hyperparameter values that leads to the highest F1-macro score across all folds.\n",
    "3. Benefits of Using GridSearchCV:\n",
    "\n",
    "Systematic Approach: Ensures that a variety of hyperparameter settings are explored, reducing the risk of missing good options.\n",
    "Automates Hyperparameter Tuning: Saves time and effort compared to manual experimentation.\n",
    "Improves Model Performance: By finding the best hyperparameter settings, you can significantly enhance the accuracy, precision, recall, or other relevant metrics for your machine learning task.\n",
    "Provides Insights: The results of the grid search can offer valuable information about how different hyperparameters affect the model's behavior.\n",
    "In essence, GridSearchCV automates the process of trying out different hyperparameter combinations and identifying the best configuration for your machine learning model, ultimately leading to better performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.fit(X_train,y_train) What it does explain in detail\n",
    "\n",
    "The line clf.fit(X_train, y_train) performs a crucial step in machine learning: training the model. Here's a breakdown of what happens:\n",
    "\n",
    "1. Input:\n",
    "\n",
    "clf: This refers to the machine learning model you've created using a specific algorithm (e.g., LogisticRegression, RandomForestClassifier, etc.). In your case, clf likely represents a classifier object you've defined earlier using GridSearchCV.\n",
    "X_train: This is the training data, containing features or independent variables that the model will learn from.\n",
    "y_train: This is the target variable or labels that the model is trying to predict.\n",
    "2. Process:\n",
    "\n",
    "Learning from Data: The model takes the training data (X_train and y_train) as input and starts to identify patterns and relationships between the features and the target variable.\n",
    "Internal Parameter Adjustments: Based on the chosen algorithm, the model iteratively adjusts its internal parameters to minimize the prediction error on the training data. These parameters essentially represent the model's \"knowledge\" about the data.\n",
    "Analogy: Imagine you're training a student to identify different types of flowers. You show them pictures of various flowers (training data) and tell them the corresponding names (target variable). The student learns by analyzing the features of the flowers (color, shape, etc.) and tries to associate them with the correct names.\n",
    "\n",
    "3. Outcome:\n",
    "\n",
    "Trained Model: After processing the training data, the model becomes equipped to make predictions for new, unseen data. It essentially learns a mapping between the features and the target variable.\n",
    "Important Notes:\n",
    "\n",
    "The specific details of how the model learns and adjusts its parameters depend on the chosen algorithm.\n",
    "The quality of the training data (X_train and y_train) significantly impacts the model's performance.\n",
    "Training is an iterative process that may involve multiple passes through the training data until the model converges.\n",
    "In essence, clf.fit(X_train, y_train) is the heart of machine learning, where the model acquires the knowledge necessary to make predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files\\python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\program files\\python38\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\program files\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\program files\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\program files\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.83452427        nan 0.84734635        nan 0.8450273\n",
      "        nan 0.84598351        nan 0.84987384]\n",
      "  warnings.warn(\n",
      "c:\\program files\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]),\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]),\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=GridSearchCV(log_class,grid,cv=cv,n_jobs=-1,scoring='f1_macro')\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\krish naik\\anaconda3\\envs\\myenv1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=GridSearchCV(log_class,grid,cv=cv,n_jobs=-1,scoring='f1_macro')\n",
    "clf.fit(X_train,y_train) \n",
    "#If i considered f1 weighted we shd be using it for kind of imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85253    54]\n",
      " [   35   101]]\n",
      "0.9989583699074237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.65      0.74      0.69       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.83      0.87      0.85     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "#Always remember for imbalanced dataset if u r getting better accuracy think that there is something wrong always do not beleive on to that.\n",
    "#If ur accuracy is high that doesn't mean that ur model is good so u have to look onto other features like 'precision', 'recall',other performance metrics\n",
    "#See we got the accuracy 0.9989583699074237\n",
    "#1 is basically fradulent info is being able to give us somewhere around 70 to 72%\n",
    "#We have 35,54(False Negative and False positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34700"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "347*100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199008\n",
       "1       356\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 100}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict({0:1,1:100})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier?\n",
    "\n",
    "The code from sklearn.ensemble import RandomForestClassifier imports a classifier class named RandomForestClassifier from the ensemble submodule of scikit-learn library.\n",
    "\n",
    "Here's a breakdown of what it means:\n",
    "\n",
    "scikit-learn: This is a popular open-source machine learning library in Python that provides a wide range of tools and algorithms for various machine learning tasks.\n",
    "ensemble: This submodule within scikit-learn focuses on ensemble methods, which are a type of machine learning technique that combines multiple weak learners (models) to create a stronger overall learner.\n",
    "RandomForestClassifier: This class specifically implements the Random Forest algorithm, which is a popular ensemble method that uses a collection of decision trees for classification tasks.\n",
    "By importing RandomForestClassifier, you gain access to the functionality of creating and using Random Forest models for classification problems in your Python code.\n",
    "\n",
    "Here are some key characteristics of Random Forest Classifiers:\n",
    "\n",
    "Ensemble of Decision Trees: It trains multiple decision trees on random subsets of features and data samples.\n",
    "Majority Vote: Predictions are made by aggregating the predictions of the individual decision trees (often through majority voting).\n",
    "Robust to Overfitting: Random Forests are generally less prone to overfitting the training data compared to single decision trees.\n",
    "Effective Feature Selection: The importance of features can be assessed based on how often they contribute to splits in the decision trees.\n",
    "If you're looking to build a classification model using the Random Forest algorithm in Python, importing RandomForestClassifier from scikit-learn is a common starting point."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classifier=RandomForestClassifier(class_weight=class_weight) Explain this in detail\n",
    "\n",
    "The code classifier = RandomForestClassifier(class_weight=class_weight) creates a Random Forest Classifier object for classification tasks in scikit-learn, specifying class_weight to potentially address class imbalance. Here's a breakdown:\n",
    "\n",
    "RandomForestClassifier:\n",
    "\n",
    "This part creates a classifier object using the RandomForestClassifier class from scikit-learn.\n",
    "Random Forest is an ensemble method that combines multiple decision trees for improved performance and reduced overfitting.\n",
    "class_weight=class_weight:\n",
    "\n",
    "This argument specifies a dictionary named class_weight that defines weights for different classes during training.\n",
    "The value of class_weight is assumed to be already defined as a dictionary elsewhere in your code (e.g., class_weight=dict({0:1,1:100})).\n",
    "Impact of class_weight:\n",
    "\n",
    "The class_weight dictionary maps class labels (integers representing categories) to their corresponding weights.\n",
    "In a balanced dataset with equal numbers of examples for each class, all weights would be 1.\n",
    "However, if your data is imbalanced (one class has significantly fewer examples), you can use class_weight to adjust learning:\n",
    "Higher weight: A class with a higher weight (e.g., 100) becomes more important for the model to learn from.\n",
    "Lower weight: A class with a lower weight (e.g., 1) has less impact on the model's learning process.\n",
    "Why Address Class Imbalance?\n",
    "\n",
    "Without class weights, a model might be biased towards the majority class, leading to poor performance on the minority class.\n",
    "Example (class_weight=dict({0:1,1:100})):\n",
    "\n",
    "Here, class 0 has a weight of 1, and class 1 has a weight of 100.\n",
    "This suggests your data likely has fewer examples of class 1 compared to class 0.\n",
    "By assigning a higher weight to class 1, the model pays more attention to these scarcer examples during training, aiming to improve its ability to classify them correctly.\n",
    "Overall, classifier=RandomForestClassifier(class_weight=class_weight) creates a Random Forest classifier that leverages class weights to potentially mitigate the effects of class imbalance in your data, leading to a more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight=dict({0:1,1:100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight={0: 1, 1: 100})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={0: 1, 1: 100})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 1, 1: 100})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier(class_weight=class_weight)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85299     8]\n",
      " [   32   104]]\n",
      "0.9995318516437859\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.93      0.76      0.84       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.96      0.88      0.92     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "#Before we have 35,54 and now we get 32,8(False -ve,False +ve)\n",
    "#Now u have increase to 104 this is pretty much good so that is y usually decision trees will not get much impact "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under Sampling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "Counter(y_train) Explain this in detail?\n",
    "\n",
    "The code from collections import Counter; Counter(y_train) utilizes the Counter class from the collections module in Python to analyze the distribution of labels in your training data (y_train). Here's a breakdown of what it does:\n",
    "\n",
    "Import Counter:\n",
    "\n",
    "The from collections import Counter line imports the Counter class from the collections module.\n",
    "Counter(y_train):\n",
    "\n",
    "This part creates a Counter object specifically for the data in the variable y_train.\n",
    "y_train is likely an array or list containing the target labels or classes for your training data.\n",
    "What is a Counter?\n",
    "\n",
    "Counter is a subclass of the built-in dict class in Python specifically designed for counting hashable objects.\n",
    "It efficiently keeps track of how many times each unique element appears in an iterable (like a list or array).\n",
    "What Does Counter(y_train) Do?\n",
    "\n",
    "It iterates through each element in y_train.\n",
    "For each unique label encountered, it increments a counter associated with that label in the resulting Counter object.\n",
    "Essentially, it creates a dictionary-like structure where:\n",
    "Keys: Represent the unique class labels present in y_train.\n",
    "Values: Represent the count (frequency) of each class label.\n",
    "Benefits of Using Counter:\n",
    "\n",
    "Efficient Counting: Counting occurrences is faster compared to manual loops or using dict directly.\n",
    "Clear Representation: The resulting Counter object provides a clear view of the class distribution in your training data.\n",
    "Example:\n",
    "\n",
    "Suppose y_train contains labels like [0, 1, 0, 1, 2, 1].\n",
    "\n",
    "Counter(y_train) would create a Counter object like:\n",
    "Counter({0: 2, 1: 3, 2: 1})\n",
    "Applications:\n",
    "\n",
    "Understanding class imbalance: Analyzing the counter can reveal if your data has imbalanced classes (some with significantly fewer examples).\n",
    "Feature engineering: The class distribution might influence feature selection or creation strategies.\n",
    "Data exploration: It's a quick way to get insights into the composition of your training data.\n",
    "In essence, Counter(y_train) is a concise and efficient way to analyze the distribution of class labels in your training data, providing valuable insights for various machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 199008, 1: 356})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0----->1(0 has 1 sample data)\n",
    "10000--->100(10000 has 100 sample data)\n",
    "\n",
    "Undersampling---reduce the points of the maximum labels\n",
    "#Instead of directly applying the Machine Learning Algorithm what I'll do is tht I will jst try to reduce this particular points(100).\n",
    "# There is a lot of disadvantage of using this u shd not coz there is a loss of data.\n",
    "# Probably we shd only use this for scenarios where u have very less dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\raghuram\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\program files\\python38\\lib\\site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\program files\\python38\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\program files\\python38\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program files\\python38\\lib\\site-packages (from imbalanced-learn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\n",
      "   ---------------------------------------- 0.0/257.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 92.2/257.7 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/257.7 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 257.7/257.7 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.0\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Under-Sampling with Sampling Rate:\n",
    "\n",
    "Under-sampling is a technique used to address class imbalance in datasets.\n",
    "The goal is to reduce the number of examples from the majority class(es) to create a more balanced distribution.\n",
    "A sampling rate of 80% (as specified by 0.8) would imply that the under-sampling method keeps only 80% of the original examples from the majority class(es). The remaining 20% would be removed to achieve a more balanced representation with the minority class."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "80% means will it remove 80% of values from majority values or will it consider 80% of values and 20%removing it?\n",
    "\n",
    "You're exactly right! In the context of under-sampling with a sampling rate of 80% using NearMiss(sampling_strategy=0.8), it removes 20% of the values from the majority class(es). Here's a breakdown:\n",
    "\n",
    "Under-Sampling with 80% Sampling Rate:\n",
    "\n",
    "Under-sampling aims to address class imbalance by reducing the number of examples in the majority class(es).\n",
    "When you specify a sampling rate of 80% using sampling_strategy=0.8, it means the under-sampling method keeps only 80% of the original examples from the majority class(es).\n",
    "The remaining 20% of examples from the majority class are removed.\n",
    "Analogy:\n",
    "\n",
    "Imagine you have a dataset with two classes: cats (majority) and dogs (minority). You want to under-sample the cats to achieve a more balanced distribution.\n",
    "\n",
    "With a sampling rate of 80%, the under-sampling method would:\n",
    "Select and retain 80% of the original cat examples.\n",
    "Remove the remaining 20% of cat examples.\n",
    "Impact on Class Distribution:\n",
    "\n",
    "By removing a portion of the majority class, under-sampling helps create a dataset where the majority class doesn't overwhelm the minority class. This can lead to improved model performance on the minority class, especially when dealing with classification tasks.\n",
    "\n",
    "Choosing the Right Sampling Rate:\n",
    "\n",
    "The optimal sampling rate depends on the severity of the class imbalance in your data and the characteristics of your specific problem. Here are some general considerations:\n",
    "\n",
    "Higher Sampling Rate (Lower Percentage Removed):\n",
    "Less aggressive reduction in the majority class.\n",
    "May not achieve significant balance depending on the imbalance level.\n",
    "Lower Sampling Rate (Higher Percentage Removed):\n",
    "More aggressive reduction in the majority class.\n",
    "Might lose valuable information from the majority class if overdone.\n",
    "It's often recommended to experiment with different sampling rates to find the best balance between reducing class imbalance and preserving informative data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199008\n",
       "1       356\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes before fit Counter({0: 199008, 1: 356})\n",
      "The number of classes after fit Counter({0: 445, 1: 356})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import NearMiss\n",
    "ns=NearMiss(sampling_strategy=0.8) #\n",
    "X_train_ns,y_train_ns=ns.fit_resample(X_train,y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_ns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After under sampling it then we are applying Machine Learning ALgorithm.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_train_ns,y_train_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74987 10320]\n",
      " [   15   121]]\n",
      "0.879042168463186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.94     85307\n",
      "           1       0.01      0.89      0.02       136\n",
      "\n",
      "    accuracy                           0.88     85443\n",
      "   macro avg       0.51      0.88      0.48     85443\n",
      "weighted avg       1.00      0.88      0.93     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "#It gives bad accuracy u can see ur precision is gone. There r so many errors w.r.t test dataset.\n",
    "#Never do an under sampling until and unless ur dataset is very very less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199008\n",
       "1       356\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35600"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "356*100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*Don't do under sampling u will miss lots of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The code os = RandomOverSampler(0.75) creates an object from the RandomOverSampler class in the imblearn.over_sampling library. This class is used to address class imbalance in datasets by over-sampling the minority class(es). Let's break down what this means:\n",
    "\n",
    "Class Imbalance:\n",
    "\n",
    "In machine learning datasets, class imbalance occurs when one class (the majority class) has significantly more examples than other classes (the minority class(es)).\n",
    "This imbalance can lead to models that perform poorly on the minority class, as they might not learn its characteristics effectively due to the limited data available.\n",
    "RandomOverSampler:\n",
    "\n",
    "The RandomOverSampler class from imblearn is a tool for over-sampling.\n",
    "Over-sampling replicates existing data points from the minority class to increase the number of examples and achieve a more balanced class distribution.\n",
    "How it Works (sampling_strategy=0.75):\n",
    "\n",
    "The RandomOverSampler class accepts an optional argument sampling_strategy.\n",
    "In your case, sampling_strategy=0.75 is provided. This argument defines the desired over-sampling ratio.\n",
    "An over-sampling ratio of 0.75 suggests that for each existing minority class example, the RandomOverSampler will create an additional 0.75 (or 75%) new examples by randomly replicating them.\n",
    "Example:\n",
    "\n",
    "Imagine you have a dataset with classes 'cat' (majority) and 'dog' (minority).\n",
    "With RandomOverSampler(0.75), for every existing 'dog' example, the over-sampler will create 0.75 additional copies, essentially replicating them 75% of the time.\n",
    "Impact on Class Distribution:\n",
    "\n",
    "By replicating minority class examples, over-sampling helps create a more balanced dataset where the minority class has a more significant presence.\n",
    "This can improve model performance on the minority class by providing more data for the model to learn from.\n",
    "Things to Consider with Over-Sampling:\n",
    "\n",
    "Increased Dataset Size: Over-sampling can significantly increase the size of your dataset, which might impact training time and computational resources.\n",
    "Potential for Overfitting: Replicating data points can introduce biases and potentially lead to overfitting, where the model memorizes the training data instead of learning general patterns.\n",
    "Alternative to RandomOverSampler:\n",
    "\n",
    "imblearn also provides other over-sampling techniques like SMOTE (Synthetic Minority Oversampling Technique) that create synthetic data points for the minority class instead of simple replication.\n",
    "In essence, os = RandomOverSampler(0.75) is a way to handle class imbalance by over-sampling the minority class in your data using random replication with a specific ratio. This can help improve model performance on the minority class, but be mindful of potential drawbacks like increased dataset size and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes before fit Counter({0: 199008, 1: 356})\n",
      "The number of classes after fit Counter({0: 199008, 1: 149256})\n"
     ]
    }
   ],
   "source": [
    "os=RandomOverSampler(sampling_strategy=0.75)\n",
    "X_train_ns,y_train_ns=os.fit_resample(X_train,y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_ns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_train_ns,y_train_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85294    13]\n",
      " [   30   106]]\n",
      "0.9994967405170698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.89      0.78      0.83       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.89      0.92     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "#See it is reduced(30). By dng the oversampling we reduced the False Negative(30).\n",
    "#85294=True Negatives, 106=True Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTETomek"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "The code from imblearn.combine import SMOTETomek imports the SMOTETomek class from the combine submodule of the imblearn library. This class is specifically designed to address class imbalance in datasets by combining two techniques:\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique): This technique creates synthetic data points for the minority class to increase the number of examples and achieve a more balanced class distribution. It does this by interpolating between existing minority class data points.\n",
    "Tomek Links: This technique identifies and removes noisy or redundant data points from both the majority and minority classes. It does this by finding pairs of examples from different classes that are too close together in the feature space.\n",
    "What SMOTETomek Offers:\n",
    "\n",
    "Addresses Imbalance and Noise: By combining SMOTE and Tomek Links, SMOTETomek aims to achieve a more balanced class distribution while potentially reducing noise in the data.\n",
    "Targeted Oversampling: SMOTE focuses on oversampling the minority class, specifically creating synthetic data points within the feature space defined by existing minority class examples.\n",
    "Noise Reduction: Tomek Links help identify and remove data points that might be confusing or misleading for the model, potentially improving its performance.\n",
    "Using SMOTETomek:\n",
    "\n",
    "Once you've imported SMOTETomek, you can create an object of this class and use it for data resampling:\n",
    "Python\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Define the SMOTETomek object with desired parameters\n",
    "smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=None)\n",
    "\n",
    "# Resample your data (X_train, y_train)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "Use code with caution.\n",
    "Things to Consider:\n",
    "\n",
    "The effectiveness of SMOTETomek can depend on the specific characteristics of your dataset and the chosen hyperparameters.\n",
    "Experimenting with different configurations of SMOTETomek might be necessary to find the optimal settings for your problem.\n",
    "Alternatives:\n",
    "\n",
    "imblearn offers other techniques for handling class imbalance, such as RandomOverSampler (simple replication) or various versions of SMOTE that create synthetic data points in different ways.\n",
    "In essence, from imblearn.combine import SMOTETomek provides a tool for combining oversampling and noise reduction techniques to address class imbalance in your data. This can potentially improve the performance of machine learning models on imbalanced datasets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SMOTETomek(sampling_strategy=0.75)\n",
    "\n",
    "The code SMOTETomek(sampling_strategy=0.75) creates an object of the SMOTETomek class from the imblearn.combine library. This class is designed for handling class imbalance in machine learning datasets by combining two techniques:\n",
    "\n",
    "1. SMOTE (Synthetic Minority Oversampling Technique): Over-samples the minority class by creating synthetic data points based on existing minority class examples.\n",
    "2. Tomek Links: Identifies and removes noisy or redundant data points from both majority and minority classes.\n",
    "\n",
    "Here's a breakdown of the specific configuration in your code:\n",
    "\n",
    "SMOTETomek Class: This is the core class that combines SMOTE and Tomek Links.\n",
    "sampling_strategy=0.75: This is an optional argument that defines the desired over-sampling ratio for the minority class using SMOTE.\n",
    "What sampling_strategy=0.75 Does:\n",
    "\n",
    "In SMOTETomek, the sampling_strategy argument controls the oversampling aspect handled by SMOTE.\n",
    "A value of 0.75 suggests that for each existing minority class example, SMOTETomek will create an additional 0.75 (or 75%) new synthetic data points on average.\n",
    "Overall Impact:\n",
    "\n",
    "SMOTETomek aims to achieve a more balanced class distribution by:\n",
    "Over-sampling the minority class using SMOTE (creating synthetic data points).\n",
    "Potentially reducing noise in both classes using Tomek Links.\n",
    "The specified sampling_strategy=0.75 instructs SMOTETomek to oversample the minority class with a ratio of 75%.\n",
    "Additional Notes:\n",
    "\n",
    "The effectiveness of this specific configuration (and SMOTETomek in general) depends on your dataset and the chosen hyperparameters. Experimentation might be needed to find the optimal settings.\n",
    "sampling_strategy can also be set to other values between 0 and 1, representing different oversampling ratios for the minority class. You can even use \"auto\" to let the library determine a suitable strategy.\n",
    "In essence, SMOTETomek(sampling_strategy=0.75) is a configuration for handling class imbalance by combining oversampling with noise reduction. The sampling_strategy argument controls the extent of oversampling for the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes before fit Counter({0: 199008, 1: 356})\n",
      "The number of classes after fit Counter({0: 198202, 1: 148450})\n"
     ]
    }
   ],
   "source": [
    "os=SMOTETomek(sampling_strategy=0.75)\n",
    "X_train_ns,y_train_ns=os.fit_resample(X_train,y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_ns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_train_ns,y_train_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85286    12]\n",
      " [   23   122]]\n",
      "0.9995903701883126\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85298\n",
      "           1       0.91      0.84      0.87       145\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.96      0.92      0.94     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "# "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*Over Sampling & Under Sampling applied during the model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>EasyEnsembleClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EasyEnsembleClassifier</label><div class=\"sk-toggleable__content\"><pre>EasyEnsembleClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "EasyEnsembleClassifier()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy=EasyEnsembleClassifier()\n",
    "easy.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>EasyEnsembleClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EasyEnsembleClassifier</label><div class=\"sk-toggleable__content\"><pre>EasyEnsembleClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "EasyEnsembleClassifier()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82812  2495]\n",
      " [   16   120]]\n",
      "0.9706119869386609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     85307\n",
      "           1       0.05      0.88      0.09       136\n",
      "\n",
      "    accuracy                           0.97     85443\n",
      "   macro avg       0.52      0.93      0.54     85443\n",
      "weighted avg       1.00      0.97      0.98     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=easy.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "#This is actually giving bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
